---
---
<html lang="pt">

<head>
	<!-- basic -->
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<!-- mobile metas -->
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="viewport" content="initial-scale=1, maximum-scale=1">
	<!-- site metas -->
	<title>LLMs</title>
	<meta name="keywords" content="">
	<meta name="description" content="">
	<meta name="author" content="">
	<!-- bootstrap css -->
	<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">
	<!-- style css -->
	<link rel="stylesheet" type="text/css" href="css/style.css">
	<link rel="stylesheet" type="text/css" href="css/custom.css">
	<!-- Responsive-->
	<link rel="stylesheet" href="css/responsive.css">
	<!-- fevicon -->
	<link rel="icon" href="images/fevicon.png" type="image/gif" />
	<!-- Scrollbar Custom CSS -->
	<link rel="stylesheet" href="css/jquery.mCustomScrollbar.min.css">
	<!-- Tweaks for older IEs-->
	<link rel="stylesheet" href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css">
	<!-- fonts -->
	<link href="https://fonts.googleapis.com/css?family=Poppins:400,700|Righteous&display=swap" rel="stylesheet">
	<!-- owl stylesheets -->
	<link rel="stylesheet" href="css/owl.carousel.min.css">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.css"
		media="screen">
</head>

<body>
	<!-- header section start -->
	<div class="header_section">
		{% include header.html %}
	</div>
	<!-- header section end -->
	<!-- services section start -->
	<div class="services_section layout_padding">
		<div class="container">
			<div class="row">
				<h1 class="services_taital">LLMs</h1>
			</div>
			<div class="row">
				<div class="col-xl-8 col-lg-6 col-md">
					<p class="services_text">Modelos de Linguagem de Grande Escala (LLMs, do inglês Large Language Models) são modelos de inteligência artificial treinados em vastas quantidades de dados textuais para entender e gerar linguagem humana. Esses modelos utilizam técnicas avançadas de aprendizado de máquina para capturar padrões complexos na linguagem. Eles podem realizar uma ampla variedade de tarefas relacionadas ao processamento de linguagem natural (NLP), como tradução automática, resumo de textos, respostas a perguntas e geração de conteúdo. Exemplos conhecidos de LLMs incluem o GPT-4, desenvolvido pela OpenAI, que pode gerar textos coerentes e contextualmente relevantes em diversas situações.</p>
					<p class="services_text">Esses modelos são treinados em grandes volumes de texto retirados de livros, artigos, websites e outras fontes, o que lhes permite adquirir conhecimento sobre diferentes tópicos e estilos de escrita. Apesar de sua utilidade, os LLMs também levantam questões éticas, como o potencial para gerar desinformação, viés nos dados e a necessidade de supervisão humana em suas aplicações.</p>
				</div>
				<div class="col-xl-4 col-lg-6 d-none d-lg-block my-auto">
					<div><img src="images/LLMs.jpg" class="material_image"></div>
				</div>
			</div>
			<br><br>
			<div class="row">
				<div class="col-md">
					<h2>Aplicações Móveis</h2>
					<p class="services_text">Executar LLMs em celulares é desafiador devido às limitações de recursos, como memória, poder de processamento e armazenamento, que são insuficientes para suportar modelos de grande escala. Além disso, o alto consumo de energia e o aquecimento gerado pelo processamento intensivo podem rapidamente esgotar a bateria e afetar o desempenho do dispositivo. Devido a essas limitações, a maioria das implementações de LLMs em celulares depende da computação em nuvem, onde o processamento pesado é feito remotamente.</p>
					<p class="services_text">No LuxAI, nos especializamos em vencer essas barreiras, desenvolvendo aplicações móveis com LLMs utilizando técnicas estado-da-arte, como Prompt Engineering e RAG para conseguir obter boas respostas de modelos menores, aplicados a contextos específicos, tudo executado localmente para garantir a segurança dos dados dos usuários e sem necessidade de um servidor dedicado para a IA. Essas técnicas também permitem uma flexibilidade maior da solução, já que é possível ajustar a base de conhecimento do modelo com pouco custo computacional.</p>
					<iframe class="pt-4" id="apresentacao_rag"
						src="https://docs.google.com/presentation/d/e/2PACX-1vQLr-cwuPj6F2-oXKDbjUXu4NYuG_aqa7aq4izlctXHibasN2GAPBae6P6yuVY79mtVXaRN9gjbGGKs/embed?start=false&loop=false&delayms=3000"
						frameborder="0" width="100%" allowfullscreen="true" mozallowfullscreen="true"
						webkitallowfullscreen="true"></iframe>
				</div>
			</div>
			<br><br>
			<div class="row">
				<div class="col-md">
					<h2>Validação</h2>
					<p class="services_text">As estratégias implementadas foram validadas utilizando um framework de avaliação de modelos (veja o trabalho completo na página de <b><a href="publicacoes">publicações</a></b>). O LuxAI desenvolveu uma pipeline para execução de LLMs em dispositivos móveis incorporando RAG, estruturando um fluxo de que começa com a fragmentação semântica dos dados em pequenos trechos (chunks). Em seguida, diferentes LLMs Geradores são utilizados para produzir respostas com base exclusivamente no contexto recuperado, garantindo maior fidelidade às informações disponíveis. Para avaliar a qualidade dessas respostas, foi implementado um LLM Juiz, responsável por analisar a coerência das respostas em relação ao texto de referência e atribuir notas conforme critérios pré-definidos. Essa abordagem possibilita a comparação objetiva do desempenho de múltiplos modelos, permitindo identificar a qualidade das respostas  para as perguntas do usuário, considerando um contexto específico.</p>
					<p class="services_text">Os benefícios dessa abordagem são significativos, tanto para pesquisadores quanto para usuários que precisam de respostas precisas extraídas de grandes volumes de informações. O framework oferece um meio objetivo de avaliar a eficácia da recuperação pelo RAG e a adequação das respostas ao contexto fornecido, eliminando a subjetividade da análise manual. A automação do processo de avaliação por meio do LLM Juiz reduz custos e tempo de análise, tornando a metodologia escalável para conjuntos de dados maiores.</p>
				</div>
			</div>
			<div class="row pt-5 align-self-center">
				<div class="col-md">
					<center>
						<a class="read_publication_button" href="publicacoes#pub_llm_rag"><img src="images/books-icon.svg" class="read_publication_button_image">Clique aqui para ler o relatório técnico na íntegra</a>
					</center>
				</div>
			</div>
		</div>
	</div>
	<!-- services section end -->
	<!-- footer section start -->
	{% include footer.html %}
	<!-- footer section end -->
	<!-- copyright section start -->
	{% include copyright.html %}
	<!-- copyright section end -->
	<!-- Javascript files-->
	<script>
		const resize_ob = new ResizeObserver(function (entries) {
			// Como a seleção é de apenas um elemento, pegamos o primeiro item do array e definimos um objeto "rect" com as dimensões do slide
			let rect = entries[0].contentRect;

			// Pega a largura atual do slide
			let width = rect.width;

			// Calcula a altera pela proporção do slide (16:9) e acrescenta a dimensão da barra de navegação do Google Slides (32px)
			let height = width / 16 * 9 + 32;

			// Define a altura atualizada do slide
			document.getElementById('apresentacao_rag').setAttribute('height', height)
		});

		// Observa as mudanças de dimensão
		resize_ob.observe(document.querySelector("#apresentacao_rag"));
	</script>
	<script src="js/jquery.min.js"></script>
	<script src="js/popper.min.js"></script>
	<script src="js/bootstrap.bundle.min.js"></script>
	<script src="js/jquery-3.0.0.min.js"></script>
	<script src="js/plugin.js"></script>
	<!-- sidebar -->
	<script src="js/jquery.mCustomScrollbar.concat.min.js"></script>
	<script src="js/custom.js"></script>
	<!-- javascript -->
	<script src="https:cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.js"></script>
</body>

</html>